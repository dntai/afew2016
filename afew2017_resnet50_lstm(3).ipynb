{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from tensorflow.contrib.keras.python.keras.layers.core import Dense, Dropout, Flatten\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.contrib.keras.python.keras.optimizers import RMSprop, Adadelta, Adam\n",
    "\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.contrib.keras.python.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.contrib.keras.python.keras.models import load_model\n",
    "from tensorflow.contrib.keras.python.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network\n",
    "TRAIN = 1\n",
    "n_classes = 7\n",
    "# n_classes = 6\n",
    "classes=np.array(('angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise'))\n",
    "# classes=np.array(('angry', 'disgust', 'fear', 'neutral', 'sad', 'surprise'))\n",
    "\n",
    "# data\n",
    "REBUILD_DATA = 0\n",
    "size = 224\n",
    "video_length = 48\n",
    "features_length = 2048\n",
    "\n",
    "# training\n",
    "PATH_TRAIN = 'E:/EmotiW2017/lstm/aligned/Train_AFEW_original'\n",
    "# PATH_TRAIN = 'E:/EmotiW2017/lstm/aligned/Train_AFEW_original'\n",
    "# PATH_TRAIN = 'E:/EmotiW2017/lstm/aligned/Train_AFEW_N'\n",
    "NPY_X_TRAIN_DATA = 'training_data_x_resnet50_aligned.npy'\n",
    "NPY_Y_TRAIN_DATA = 'training_data_y_resnet50_aligned.npy'\n",
    "# NPY_X_TRAIN_DATA = 'training_data_x_resnet50_aligned_aug.npy'\n",
    "# NPY_Y_TRAIN_DATA = 'training_data_y_resnet50_aligned_aug.npy'\n",
    "# NPY_X_TRAIN_DATA = 'training_data_x_inceptionv3_aligned.npy'\n",
    "# NPY_Y_TRAIN_DATA = 'training_data_y_inceptionv3_aligned.npy'\n",
    "\n",
    "# validation\n",
    "# PATH_VAL = 'E:/EmotiW2017/lstm/aligned/Val_AFEW_original'\n",
    "PATH_VAL = 'E:/EmotiW2017/lstm/aligned/Val_AFEW_original'\n",
    "NPY_X_VAL_DATA = 'val_data_x_resnet50_aligned.npy'\n",
    "NPY_Y_VAL_DATA = 'val_data_y_resnet50_aligned.npy'\n",
    "# NPY_X_VAL_DATA = 'val_data_x_inceptionv3_aligned.npy'\n",
    "# NPY_Y_VAL_DATA = 'val_data_y_inceptionv3_aligned.npy'\n",
    "\n",
    "# model\n",
    "WEIGHT_FILE = 'resnet50_lstm_aligned_3.hdf5'\n",
    "MODEL_FILE = 'resnet50_lstm_aligned_3.h5'\n",
    "TRAINING_HIST = 'resnet50_lstm_aligned_3.pickle'\n",
    "# WEIGHT_FILE = 'inceptionv3_lstm_aligned.hdf5'\n",
    "# MODEL_FILE = 'inceptionv3_lstm_aligned.h5'\n",
    "# TRAINING_HIST = 'inceptionv3_lstm_aligned.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\models.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('ResNet50_ImageNet.hdf5')\n",
    "# model = load_model('InceptionV3_ImageNet.hdf5')\n",
    "# model = InceptionV3(weights='imagenet', include_top=False)\n",
    "# model.save('InceptionV3_ImageNet.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature(img_path, model, size):\n",
    "    img = image.load_img(img_path, target_size=(size, size))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    features = model.predict(x)\n",
    "    \n",
    "    return features[0][0][0]\n",
    "\n",
    "# features = extract_feature('duong.jpg', model, size)\n",
    "# features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion to Integer Convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emo2int(emo):\n",
    "    if n_classes == 7:\n",
    "        if emo == 'Angry':\n",
    "            return 0\n",
    "        elif emo == 'Disgust':\n",
    "            return 1\n",
    "        elif emo == 'Fear':\n",
    "            return 2\n",
    "        elif emo == 'Happy':\n",
    "            return 3\n",
    "        elif emo == 'Neutral':\n",
    "            return 4\n",
    "        elif emo == 'Sad':\n",
    "            return 5\n",
    "        else:\n",
    "            return 6\n",
    "    elif n_classes == 6:\n",
    "        if emo == 'Angry':\n",
    "            return 0\n",
    "        elif emo == 'Disgust':\n",
    "            return 1\n",
    "        elif emo == 'Fear':\n",
    "            return 2\n",
    "        elif emo == 'Neutral':\n",
    "            return 3\n",
    "        elif emo == 'Sad':\n",
    "            return 4\n",
    "        else:\n",
    "            return 5\n",
    "    else:\n",
    "        if emo == 'angry_happy_neutral':\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Unnormalized confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n",
    "\n",
    "    thresh = cm.min() + (cm.max() - cm.min()) / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> loading training data...done.\n",
      "\tshapes: (726, 48, 2048), (726,)\n",
      "\ttypes:  float64, int32\n",
      "\tmemory: 544.5 MB, 0.00276947021484375 MB\n"
     ]
    }
   ],
   "source": [
    "# if REBUILD_DATA == 0 and os.path.isfile(NPY_X_TRAIN_DATA) and os.path.isfile(NPY_Y_TRAIN_DATA):\n",
    "#     print('>>> loading training data...', end='')\n",
    "#     X_train = np.load(NPY_X_TRAIN_DATA)\n",
    "#     y_train = np.load(NPY_Y_TRAIN_DATA)\n",
    "#     print('done.')\n",
    "# else:\n",
    "#     # Count # of videos\n",
    "#     n_vids_train = 0\n",
    "\n",
    "#     for emo in os.listdir(PATH_TRAIN):\n",
    "#         full_emo = PATH_TRAIN + '/' + emo\n",
    "#         if os.path.isdir(full_emo):\n",
    "#             for vid in os.listdir(full_emo):\n",
    "#                 full_vid = full_emo + '/' + vid\n",
    "\n",
    "#                 n_vids_train += 1\n",
    "    \n",
    "#     # Read images and extract features\n",
    "#     vid_indx = 0\n",
    "    \n",
    "#     X_train = np.zeros((n_vids_train, video_length, features_length))\n",
    "#     y_train = []\n",
    "\n",
    "#     for emo in os.listdir(PATH_TRAIN):\n",
    "#         full_emo = PATH_TRAIN + '/' + emo\n",
    "#         if os.path.isdir(full_emo):\n",
    "#             for vid in os.listdir(full_emo):\n",
    "#                 full_vid = full_emo + '/' + vid\n",
    "\n",
    "#                 # Extract features\n",
    "#                 fra_indx = 0\n",
    "#                 for fra in os.listdir(full_vid):\n",
    "#                     if fra_indx < video_length:\n",
    "#                         full_fra = full_vid + '/' + fra\n",
    "#                         if os.path.isfile(full_fra):\n",
    "#                             # Clear screen\n",
    "#                             clear_output(wait=True)\n",
    "#                             print('> extracting features for video #%d/%d at frame #%d/%d' % (vid_indx + 1, n_vids_train, fra_indx + 1, video_length))\n",
    "                            \n",
    "#                             features = extract_feature(full_fra, model, size)\n",
    "                            \n",
    "#                             # Add extracted features to dataset\n",
    "#                             X_train[vid_indx, fra_indx, :] = features\n",
    "\n",
    "#                             fra_indx += 1\n",
    "#                     else:\n",
    "#                         break\n",
    "\n",
    "#                 # Get the label for current video\n",
    "#                 y_train.append(emo2int(emo))\n",
    "\n",
    "#                 vid_indx += 1\n",
    "\n",
    "#     y_train = np.array(y_train)\n",
    "    \n",
    "#     clear_output(wait=True)\n",
    "#     print('>>> processed %d videos' % (n_vids_train))\n",
    "\n",
    "#     np.save(NPY_X_TRAIN_DATA, X_train)\n",
    "#     np.save(NPY_Y_TRAIN_DATA, y_train)\n",
    "\n",
    "print('>>> loading training data...', end='')\n",
    "X_train = np.load(NPY_X_TRAIN_DATA)\n",
    "y_train = np.load(NPY_Y_TRAIN_DATA)\n",
    "print('done.')\n",
    "    \n",
    "print('\\tshapes: {}, {}'.format(X_train.shape, y_train.shape))\n",
    "print('\\ttypes:  {}, {}'.format(X_train.dtype, y_train.dtype))\n",
    "print('\\tmemory: {} MB, {} MB'.format(X_train.nbytes / 1048576, y_train.nbytes / 1048576))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> loading validation data...done.\n",
      "\tshapes: (383, 48, 2048), (383,)\n",
      "\ttypes:  float64, int32\n",
      "\tmemory: 287.25 MB, 0.001461029052734375 MB\n"
     ]
    }
   ],
   "source": [
    "if REBUILD_DATA == 0 and os.path.isfile(NPY_X_VAL_DATA) and os.path.isfile(NPY_Y_VAL_DATA):\n",
    "    print('>>> loading validation data...', end='')\n",
    "    X_val = np.load(NPY_X_VAL_DATA)\n",
    "    y_val = np.load(NPY_Y_VAL_DATA)\n",
    "    print('done.')\n",
    "else:\n",
    "    # Count # of videos\n",
    "    n_vids_val = 0\n",
    "\n",
    "    for emo in os.listdir(PATH_VAL):\n",
    "        full_emo = PATH_VAL + '/' + emo\n",
    "        if os.path.isdir(full_emo):\n",
    "            for vid in os.listdir(full_emo):\n",
    "                full_vid = full_emo + '/' + vid\n",
    "\n",
    "                n_vids_val += 1\n",
    "    \n",
    "    # Read images and extract features\n",
    "    vid_indx = 0\n",
    "    \n",
    "    X_val = np.zeros((n_vids_val, video_length, features_length))\n",
    "    y_val = []\n",
    "\n",
    "    for emo in os.listdir(PATH_VAL):\n",
    "        full_emo = PATH_VAL + '/' + emo\n",
    "        if os.path.isdir(full_emo):\n",
    "            for vid in os.listdir(full_emo):\n",
    "                full_vid = full_emo + '/' + vid\n",
    "\n",
    "                # Extract features\n",
    "                fra_indx = 0\n",
    "                for fra in os.listdir(full_vid):\n",
    "                    if fra_indx < video_length:\n",
    "                        full_fra = full_vid + '/' + fra\n",
    "                        if os.path.isfile(full_fra):\n",
    "                            # Clear screen\n",
    "                            clear_output(wait=True)\n",
    "                            print('> extracting features for video #%d/%d at frame #%d/%d' % (vid_indx + 1, n_vids_val, fra_indx + 1, video_length))\n",
    "                            \n",
    "                            features = extract_feature(full_fra, model, size)\n",
    "                            \n",
    "                            # Add extracted features to dataset\n",
    "                            X_val[vid_indx, fra_indx, :] = features\n",
    "\n",
    "                            fra_indx += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # Get the label for current video\n",
    "                y_val.append(emo2int(emo))\n",
    "\n",
    "                vid_indx += 1\n",
    "\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('>>> processed %d videos' % (n_vids_val))\n",
    "\n",
    "    np.save(NPY_X_VAL_DATA, X_val)\n",
    "    np.save(NPY_Y_VAL_DATA, y_val)\n",
    "    \n",
    "print('\\tshapes: {}, {}'.format(X_val.shape, y_val.shape))\n",
    "print('\\ttypes:  {}, {}'.format(X_val.dtype, y_val.dtype))\n",
    "print('\\tmemory: {} MB, {} MB'.format(X_val.nbytes / 1048576, y_val.nbytes / 1048576))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, input_shape=(video_length, features_length),\n",
    "               dropout=0.5, return_sequences=False))\n",
    "# lstm.add(LSTM(2048, input_shape=(video_length, features_length),\n",
    "#                dropout=0.5, return_sequences=False))\n",
    "# lstm.add(Flatten())\n",
    "lstm.add(Dense(256, activation='relu'))\n",
    "lstm.add(Dropout(0.5))\n",
    "lstm.add(Dense(256, activation='relu'))\n",
    "lstm.add(Dropout(0.5))\n",
    "lstm.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               1114624   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 1,215,239.0\n",
      "Trainable params: 1,215,239.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 726 samples, validate on 383 samples\n",
      "Epoch 1/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 2.0007 - acc: 0.1701Epoch 00000: val_acc improved from -inf to 0.15666, saving model to checkpoints/cnn_images_224_model.00000-2.0003-0.1667-1.9450-0.1567.hdf5\n",
      "726/726 [==============================] - 2s - loss: 2.0003 - acc: 0.1667 - val_loss: 1.9450 - val_acc: 0.1567\n",
      "Epoch 2/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9709 - acc: 0.1788Epoch 00001: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9663 - acc: 0.1873 - val_loss: 1.9373 - val_acc: 0.1436\n",
      "Epoch 3/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9731 - acc: 0.1483Epoch 00002: val_acc improved from 0.15666 to 0.17493, saving model to checkpoints/cnn_images_224_model.00002-1.9754-0.1446-1.9302-0.1749.hdf5\n",
      "726/726 [==============================] - 1s - loss: 1.9754 - acc: 0.1446 - val_loss: 1.9302 - val_acc: 0.1749\n",
      "Epoch 4/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9658 - acc: 0.1512Epoch 00003: val_acc improved from 0.17493 to 0.20366, saving model to checkpoints/cnn_images_224_model.00003-1.9643-0.1501-1.9262-0.2037.hdf5\n",
      "726/726 [==============================] - 1s - loss: 1.9643 - acc: 0.1501 - val_loss: 1.9262 - val_acc: 0.2037\n",
      "Epoch 5/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9498 - acc: 0.1802Epoch 00004: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9487 - acc: 0.1777 - val_loss: 1.9235 - val_acc: 0.1958\n",
      "Epoch 6/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9451 - acc: 0.1759Epoch 00005: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9460 - acc: 0.1736 - val_loss: 1.9217 - val_acc: 0.1749\n",
      "Epoch 7/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9432 - acc: 0.1948Epoch 00006: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9452 - acc: 0.1942 - val_loss: 1.9201 - val_acc: 0.1932\n",
      "Epoch 8/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9403 - acc: 0.1890Epoch 00007: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9425 - acc: 0.1928 - val_loss: 1.9195 - val_acc: 0.1854\n",
      "Epoch 9/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9273 - acc: 0.1860Epoch 00008: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9261 - acc: 0.1873 - val_loss: 1.9180 - val_acc: 0.1828\n",
      "Epoch 10/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9227 - acc: 0.2282Epoch 00009: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9243 - acc: 0.2245 - val_loss: 1.9164 - val_acc: 0.1880\n",
      "Epoch 11/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9293 - acc: 0.1948Epoch 00010: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9304 - acc: 0.1928 - val_loss: 1.9152 - val_acc: 0.1880\n",
      "Epoch 12/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9334 - acc: 0.1991Epoch 00011: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9273 - acc: 0.2011 - val_loss: 1.9132 - val_acc: 0.1880\n",
      "Epoch 13/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9183 - acc: 0.1948Epoch 00012: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9192 - acc: 0.1928 - val_loss: 1.9118 - val_acc: 0.1775\n",
      "Epoch 14/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9148 - acc: 0.1875Epoch 00013: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9152 - acc: 0.1832 - val_loss: 1.9114 - val_acc: 0.1828\n",
      "Epoch 15/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9183 - acc: 0.2035Epoch 00014: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9164 - acc: 0.2066 - val_loss: 1.9097 - val_acc: 0.1802\n",
      "Epoch 16/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9015 - acc: 0.2355Epoch 00015: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9065 - acc: 0.2314 - val_loss: 1.9093 - val_acc: 0.1828\n",
      "Epoch 17/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9114 - acc: 0.2035Epoch 00016: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9131 - acc: 0.2011 - val_loss: 1.9097 - val_acc: 0.1932\n",
      "Epoch 18/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9061 - acc: 0.2006Epoch 00017: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9092 - acc: 0.2011 - val_loss: 1.9087 - val_acc: 0.1932\n",
      "Epoch 19/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9144 - acc: 0.2035Epoch 00018: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9152 - acc: 0.2039 - val_loss: 1.9083 - val_acc: 0.1880\n",
      "Epoch 20/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9115 - acc: 0.1991Epoch 00019: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9110 - acc: 0.1983 - val_loss: 1.9079 - val_acc: 0.1932\n",
      "Epoch 21/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9010 - acc: 0.2151Epoch 00020: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9020 - acc: 0.2121 - val_loss: 1.9073 - val_acc: 0.1984\n",
      "Epoch 22/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8983 - acc: 0.2485Epoch 00021: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8981 - acc: 0.2466 - val_loss: 1.9062 - val_acc: 0.1984\n",
      "Epoch 23/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9151 - acc: 0.2064Epoch 00022: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9127 - acc: 0.2039 - val_loss: 1.9056 - val_acc: 0.1932\n",
      "Epoch 24/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9037 - acc: 0.2151Epoch 00023: val_acc improved from 0.20366 to 0.21149, saving model to checkpoints/cnn_images_224_model.00023-1.9016-0.2190-1.9053-0.2115.hdf5\n",
      "726/726 [==============================] - 1s - loss: 1.9016 - acc: 0.2190 - val_loss: 1.9053 - val_acc: 0.2115\n",
      "Epoch 25/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9003 - acc: 0.2108Epoch 00024: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9023 - acc: 0.2066 - val_loss: 1.9045 - val_acc: 0.2010\n",
      "Epoch 26/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8851 - acc: 0.2253Epoch 00025: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8851 - acc: 0.2231 - val_loss: 1.9041 - val_acc: 0.1984\n",
      "Epoch 27/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8939 - acc: 0.2209Epoch 00026: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8931 - acc: 0.2204 - val_loss: 1.9039 - val_acc: 0.2010\n",
      "Epoch 28/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8868 - acc: 0.2224- ETA: 0s - loss: 1.8942 - acc: 0.226Epoch 00027: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8859 - acc: 0.2259 - val_loss: 1.9032 - val_acc: 0.2063\n",
      "Epoch 29/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9112 - acc: 0.2108Epoch 00028: val_acc improved from 0.21149 to 0.21410, saving model to checkpoints/cnn_images_224_model.00028-1.9164-0.2121-1.9020-0.2141.hdf5\n",
      "726/726 [==============================] - 1s - loss: 1.9164 - acc: 0.2121 - val_loss: 1.9020 - val_acc: 0.2141\n",
      "Epoch 30/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8916 - acc: 0.2137Epoch 00029: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8876 - acc: 0.2218 - val_loss: 1.9023 - val_acc: 0.2089\n",
      "Epoch 31/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8908 - acc: 0.2297Epoch 00030: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8915 - acc: 0.2314 - val_loss: 1.9029 - val_acc: 0.2063\n",
      "Epoch 32/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8898 - acc: 0.2020Epoch 00031: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8879 - acc: 0.2107 - val_loss: 1.9029 - val_acc: 0.1958\n",
      "Epoch 33/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8805 - acc: 0.2326Epoch 00032: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8837 - acc: 0.2287 - val_loss: 1.9013 - val_acc: 0.2037\n",
      "Epoch 34/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9000 - acc: 0.1977Epoch 00033: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9054 - acc: 0.1928 - val_loss: 1.9014 - val_acc: 0.2037\n",
      "Epoch 35/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.9049 - acc: 0.2340Epoch 00034: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.9076 - acc: 0.2287 - val_loss: 1.9020 - val_acc: 0.1984\n",
      "Epoch 36/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8917 - acc: 0.2049Epoch 00035: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8957 - acc: 0.2039 - val_loss: 1.9017 - val_acc: 0.2037\n",
      "Epoch 37/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8833 - acc: 0.2398Epoch 00036: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8890 - acc: 0.2355 - val_loss: 1.9008 - val_acc: 0.2037\n",
      "Epoch 38/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8886 - acc: 0.2384Epoch 00037: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8860 - acc: 0.2369 - val_loss: 1.9001 - val_acc: 0.1932\n",
      "Epoch 39/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8855 - acc: 0.2587Epoch 00038: val_acc did not improve\n",
      "726/726 [==============================] - 1s - loss: 1.8825 - acc: 0.2548 - val_loss: 1.9005 - val_acc: 0.1958\n",
      "Epoch 40/10000\n",
      "688/726 [===========================>..] - ETA: 0s - loss: 1.8724 - acc: 0.2224"
     ]
    }
   ],
   "source": [
    "batch_size = 172\n",
    "n_epochs = 10000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "early_stop_after = 50\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=early_stop_after, verbose=1)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', filepath='checkpoints/cnn_images' + '_' + str(size) + '_model.{epoch:05d}-{loss:.4f}-{acc:.4f}-{val_loss:.4f}-{val_acc:.4f}.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "# opt = Adam(lr=learning_rate)\n",
    "opt = RMSprop(lr=learning_rate, decay=10**-6)\n",
    "lstm.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "if TRAIN == 1:\n",
    "    train_history = lstm.fit(X_train, y_train,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             shuffle=True, batch_size=batch_size,\n",
    "                             epochs=n_epochs,\n",
    "                             verbose=1,\n",
    "                             callbacks=[checkpointer, early_stopping])\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print('> Training time: ' + str(training_time) + ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model OR Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN == 1:\n",
    "    print('> Saving trained model...', end='')\n",
    "    # Save trained model\n",
    "    lstm.save_weights(WEIGHT_FILE)\n",
    "    lstm.save(MODEL_FILE)\n",
    "    # Save training history\n",
    "    with open(TRAINING_HIST, 'wb') as f:\n",
    "        pickle.dump(train_history.history, f)\n",
    "        \n",
    "    history_ = train_history.history\n",
    "    print('done.')\n",
    "else:\n",
    "    print('> Loading trained model...', end='')\n",
    "    # Load trained model\n",
    "    lstm.load_weights(WEIGHT_FILE)\n",
    "#     lstm = load_model(MODEL_FILE)\n",
    "    \n",
    "    # Load training history\n",
    "    file = open(TRAINING_HIST, 'rb')\n",
    "    history_ = pickle.load(file)\n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest weight file based on last modified time\n",
    "lowf = glob.glob('./checkpoints/*.hdf5')\n",
    "BEST_WEIGHT_FILE = max(lowf, key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> loading trained model...', end='')\n",
    "# Load trained model\n",
    "# lstm = load_model(BEST_WEIGHT_FILE)\n",
    "lstm.load_weights(BEST_WEIGHT_FILE)\n",
    "\n",
    "# Load training history\n",
    "file = open(TRAINING_HIST, 'rb')\n",
    "history_ = pickle.load(file)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate model again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = lstm.evaluate(X_train, y_train)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = lstm.evaluate(X_val, y_val)\n",
    "print('Val score:', score[0])\n",
    "print('Val accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_training_ = lstm.predict(x=X_train, batch_size=batch_size, verbose=1)\n",
    "y_pred_ = lstm.predict(x=X_val, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count # of videos\n",
    "n_vids_val = 0\n",
    "\n",
    "PATH_VAL_CSV = 'E:/EmotiW2017/lstm/aligned/Val_AFEW_original_scores'\n",
    "\n",
    "for emo in os.listdir(PATH_VAL):\n",
    "    full_emo = PATH_VAL + '/' + emo\n",
    "    if os.path.isdir(full_emo):\n",
    "        for vid in os.listdir(full_emo):\n",
    "            n_vids_val += 1\n",
    "\n",
    "# Read images and extract features\n",
    "vid_indx = 0\n",
    "\n",
    "val_names = []\n",
    "\n",
    "for emo in os.listdir(PATH_VAL):\n",
    "    full_emo = PATH_VAL + '/' + emo\n",
    "    if os.path.isdir(full_emo):\n",
    "        for vid in os.listdir(full_emo):\n",
    "            full_vid = PATH_VAL_CSV + '/' + emo + '/' + vid\n",
    "            \n",
    "            full_vid = full_vid[:-14] + 'csv'\n",
    "\n",
    "            # Get the name for current video\n",
    "            val_names.append(full_vid)\n",
    "\n",
    "            vid_indx += 1\n",
    "\n",
    "# val_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_indx = 0\n",
    "cor_pred = 0\n",
    "\n",
    "for val_f_name in val_names:\n",
    "    val_scores = y_pred_[name_indx, :]\n",
    "    cor_pred += (np.argmax(y_pred_[name_indx, :]) == y_val[name_indx])\n",
    "    \n",
    "    np.savetxt(val_f_name, val_scores)\n",
    "    \n",
    "    name_indx += 1\n",
    "    \n",
    "cor_pred/len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count # of videos\n",
    "n_vids_train = 0\n",
    "\n",
    "PATH_TRAIN_CSV = 'E:/EmotiW2017/lstm/aligned/Train_AFEW_original_scores'\n",
    "\n",
    "for emo in os.listdir(PATH_TRAIN):\n",
    "    full_emo = PATH_TRAIN + '/' + emo\n",
    "    if os.path.isdir(full_emo):\n",
    "        for vid in os.listdir(full_emo):\n",
    "            n_vids_train += 1\n",
    "\n",
    "# Read images and extract features\n",
    "vid_indx = 0\n",
    "\n",
    "train_names = []\n",
    "\n",
    "for emo in os.listdir(PATH_TRAIN):\n",
    "    full_emo = PATH_TRAIN + '/' + emo\n",
    "    if os.path.isdir(full_emo):\n",
    "        for vid in os.listdir(full_emo):\n",
    "            full_vid = PATH_TRAIN_CSV + '/' + emo + '/' + vid\n",
    "            \n",
    "            full_vid = full_vid[:-14] + 'csv'\n",
    "\n",
    "            # Get the name for current video\n",
    "            train_names.append(full_vid)\n",
    "\n",
    "            vid_indx += 1\n",
    "\n",
    "# train_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_indx = 0\n",
    "cor_pred = 0\n",
    "\n",
    "for train_f_name in train_names:\n",
    "    train_scores = y_pred_training_[name_indx, :]\n",
    "    cor_pred += (np.argmax(y_pred_training_[name_indx, :]) == y_train[name_indx])\n",
    "    \n",
    "    np.savetxt(train_f_name, train_scores)\n",
    "    \n",
    "    name_indx += 1\n",
    "        \n",
    "cor_pred/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred_training = np.argmax(y_pred_training_, axis=1)\n",
    "y_pred = np.argmax(y_pred_, axis=1)\n",
    "\n",
    "plot_confusion_matrix(y_test=y_train, y_pred=y_pred_training,\n",
    "                      classes=classes,\n",
    "                      normalize=True,\n",
    "                      title='Normalized confusion matrix - training dataset',\n",
    "                      cmap=plt.cm.Reds)\n",
    "\n",
    "plot_confusion_matrix(y_test=y_val, y_pred=y_pred,\n",
    "                      classes=classes,\n",
    "                      normalize=True,\n",
    "                      title='Normalized confusion matrix - validation dataset')\n",
    "\n",
    "plot_confusion_matrix(y_test=y_val, y_pred=y_pred,\n",
    "                      classes=classes,\n",
    "                      normalize=False,\n",
    "                      title='Unnormalized confusion matrix - validation dataset')\n",
    "\n",
    "np.sum(y_pred == y_val)/len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_['acc'])\n",
    "plt.plot(history_['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_['loss'])\n",
    "plt.plot(history_['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
