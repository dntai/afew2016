{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition\n",
    "Kaggle Facial Expression Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras for TensorFlow\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\n",
    "from tensorflow.contrib.keras.python.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.contrib.keras.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.contrib.keras.python.keras.regularizers import l1, l2\n",
    "# from tensorflow.contrib.keras.python.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2304\n",
    "n_classes = 7\n",
    "img_dim = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emotion(ohv):\n",
    "    if ohv.shape[0] == 1:\n",
    "        indx = ohv[0]\n",
    "    else:\n",
    "        indx = np.argmax(ohv)\n",
    "        \n",
    "    if indx == 0:\n",
    "        return 'angry'\n",
    "    elif indx == 1:\n",
    "        return 'disgust'\n",
    "    elif indx == 2:\n",
    "        return 'fear'\n",
    "    elif indx == 3:\n",
    "        return 'happy'\n",
    "    elif indx == 4:\n",
    "        return 'sad'\n",
    "    elif indx == 5:\n",
    "        return 'surprise'\n",
    "    elif indx == 6:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_emotion_from_image(model, img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.resize(img, (48, 48))\n",
    "    img = img.reshape([1, 48, 48, 1])\n",
    "    \n",
    "    pred_cls = model.predict_classes(img, verbose=0)\n",
    "    proba = model.predict(img, verbose=0)\n",
    "    \n",
    "    return np.max(proba), pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape = (48, 48, 1)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001, decay=10e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('fer2013_weights_.h5')\n",
    "print('> model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Face Detector\n",
    "https://github.com/peiyunh/tiny  \n",
    "https://github.com/chinakook/hr101_mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "import pylab as pl\n",
    "from collections import namedtuple\n",
    "import time\n",
    "Batch = namedtuple('Batch', ['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_DIM=5000.0\n",
    "prob_thresh = 0.5\n",
    "nms_thresh = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadmeta(matpath):\n",
    "    f = sio.loadmat(matpath)\n",
    "    net = f['net']\n",
    "    clusters = np.copy(net['meta'][0][0][0][0][6])\n",
    "    averageImage = np.copy(net['meta'][0][0][0][0][2][0][0][2])\n",
    "    averageImage = averageImage[:, np.newaxis]\n",
    "    return clusters, averageImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nms(dets, prob_thresh):\n",
    "    x1 = dets[:, 0]  \n",
    "    y1 = dets[:, 1]  \n",
    "    x2 = dets[:, 2]  \n",
    "    y2 = dets[:, 3]  \n",
    "    scores = dets[:, 4]  \n",
    "    \n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)  \n",
    "\n",
    "    order = scores.argsort()[::-1]  \n",
    "\n",
    "    keep = []  \n",
    "    while order.size > 0: \n",
    "        i = order[0]  \n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])  \n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])  \n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])  \n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])  \n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)  \n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)  \n",
    "        inter = w * h\n",
    "        \n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        inds = np.where(ovr <= prob_thresh)[0]  \n",
    "\n",
    "        order = order[inds + 1]  \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters, averageImage = loadmeta('./hr_res101.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters_h = clusters[:,3] - clusters[:,1] + 1\n",
    "clusters_w = clusters[:,2] - clusters[:,0] + 1\n",
    "normal_idx = np.where(clusters[:,4] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tiny_face_detector(raw_img):\n",
    "    raw_h = raw_img.shape[0]\n",
    "    raw_w = raw_img.shape[1]\n",
    "    raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)\n",
    "    raw_img_f = raw_img.astype(np.float32)\n",
    "\n",
    "    min_scale = min(np.floor(np.log2(np.max(clusters_w[normal_idx]/raw_w))), np.floor(np.log2(np.max(clusters_h[normal_idx]/raw_h))))\n",
    "    max_scale = min(1.0, -np.log2(max(raw_h, raw_w)/MAX_INPUT_DIM))\n",
    "    \n",
    "    scales_down = pl.frange(min_scale, 0, 1.)\n",
    "    scales_up = pl.frange(0.5, max_scale,0.5)\n",
    "    scales_pow = np.hstack((scales_down, scales_up))\n",
    "    scales = np.power(2.0, scales_pow)\n",
    "    \n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint('hr101',0)\n",
    "    all_layers = sym.get_internals()\n",
    "    \n",
    "    context=mx.gpu()\n",
    "    \n",
    "    mod = mx.mod.Module(symbol=all_layers['fusex_output'], context=context, data_names=['data'], label_names=None)\n",
    "    mod.bind(for_training=False, data_shapes=[('data', (1, 3, 224, 224))], label_shapes=None, force_rebind=False)\n",
    "    mod.set_params(arg_params=arg_params, aux_params=aux_params, force_init=False)\n",
    "    \n",
    "    start = time.time()\n",
    "    bboxes = np.empty(shape=(0,5))\n",
    "    for s in scales:\n",
    "        img = cv2.resize(raw_img_f, (0,0), fx = s, fy = s)\n",
    "        img = np.transpose(img,(2,0,1))\n",
    "        img = img - averageImage\n",
    "\n",
    "        tids = []\n",
    "        if s <= 1. :\n",
    "            tids = list(range(4, 12))\n",
    "        else :\n",
    "            tids = list(range(4, 12)) + list(range(18, 25))\n",
    "        ignoredTids = list(set(range(0,clusters.shape[0]))-set(tids))\n",
    "        img_h = img.shape[1]\n",
    "        img_w = img.shape[2]\n",
    "        img = img[np.newaxis, :]\n",
    "\n",
    "        mod.reshape(data_shapes=[('data', (1, 3, img_h, img_w))])\n",
    "        mod.forward(Batch([mx.nd.array(img)]))\n",
    "        mod.get_outputs()[0].wait_to_read()\n",
    "        fusex_res = mod.get_outputs()[0]\n",
    "\n",
    "        score_cls = mx.nd.slice_axis(fusex_res, axis=1, begin=0, end=25, name='score_cls')\n",
    "        score_reg = mx.nd.slice_axis(fusex_res, axis=1, begin=25, end=None, name='score_reg')\n",
    "        prob_cls = mx.nd.sigmoid(score_cls)\n",
    "\n",
    "        prob_cls_np = prob_cls.asnumpy()\n",
    "        prob_cls_np[0,ignoredTids,:,:] = 0.\n",
    "\n",
    "        _, fc, fy, fx = np.where(prob_cls_np > prob_thresh)\n",
    "\n",
    "        cy = fy * 8 - 1\n",
    "        cx = fx * 8 - 1\n",
    "        ch = clusters[fc, 3] - clusters[fc,1] + 1\n",
    "        cw = clusters[fc, 2] - clusters[fc, 0] + 1\n",
    "\n",
    "        Nt = clusters.shape[0]\n",
    "\n",
    "        score_reg_np = score_reg.asnumpy()\n",
    "        tx = score_reg_np[0, 0:Nt, :, :]\n",
    "        ty = score_reg_np[0, Nt:2*Nt,:,:]\n",
    "        tw = score_reg_np[0, 2*Nt:3*Nt,:,:]\n",
    "        th = score_reg_np[0,3*Nt:4*Nt,:,:]\n",
    "\n",
    "        dcx = cw * tx[fc, fy, fx]\n",
    "        dcy = ch * ty[fc, fy, fx]\n",
    "        rcx = cx + dcx\n",
    "        rcy = cy + dcy\n",
    "        rcw = cw * np.exp(tw[fc, fy, fx])\n",
    "        rch = ch * np.exp(th[fc, fy, fx])\n",
    "\n",
    "        score_cls_np = score_cls.asnumpy()\n",
    "        scores = score_cls_np[0, fc, fy, fx]\n",
    "\n",
    "        tmp_bboxes = np.vstack((rcx-rcw/2, rcy-rch/2, rcx+rcw/2,rcy+rch/2))\n",
    "        tmp_bboxes = np.vstack((tmp_bboxes/s, scores))\n",
    "        tmp_bboxes = tmp_bboxes.transpose()\n",
    "        bboxes = np.vstack((bboxes, tmp_bboxes))\n",
    "    \n",
    "    print (\"time\", time.time()-start, \"secs.\")\n",
    "    refind_idx = nms(bboxes, nms_thresh)\n",
    "    refind_bboxes = bboxes[refind_idx]\n",
    "    refind_bboxes = refind_bboxes.astype(np.int32)\n",
    "    \n",
    "#     bboxes.shape\n",
    "    \n",
    "    return refind_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_img = cv2.imread('./hard.jpg')\n",
    "\n",
    "# refind_bboxes = tiny_face_detector(raw_img)\n",
    "\n",
    "# for r in refind_bboxes:\n",
    "# #     plt.imshow(cv2.cvtColor(raw_img[r[1]:r[3], r[0]:r[2]], cv2.COLOR_BGR2RGB))\n",
    "# #     plt.show()\n",
    "    \n",
    "#     cv2.rectangle(raw_img, (r[0],r[1]), (r[2],r[3]), (0,255,0),3)\n",
    "    \n",
    "# plt.imshow(cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minsize = 20\n",
    "\n",
    "caffe_model_path = \"./model\"\n",
    "\n",
    "threshold = [0.6, 0.7, 0.7]\n",
    "factor = 0.709\n",
    "\n",
    "#     caffe.set_mode_cpu()\n",
    "caffe.set_mode_gpu()\n",
    "PNet = caffe.Net(caffe_model_path+\"/det1.prototxt\", caffe_model_path+\"/det1.caffemodel\", caffe.TEST)\n",
    "RNet = caffe.Net(caffe_model_path+\"/det2.prototxt\", caffe_model_path+\"/det2.caffemodel\", caffe.TEST)\n",
    "ONet = caffe.Net(caffe_model_path+\"/det3.prototxt\", caffe_model_path+\"/det3.caffemodel\", caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "filename = 'D:/nhduong/MotionRecognition/data/AFEW/AFEW_6_2016/Val/Data/Fear/000142325.avi.mp4'\n",
    "vid = imageio.get_reader(filename, 'ffmpeg')\n",
    "\n",
    "emo_count = np.zeros((7, 1))\n",
    "\n",
    "for num, raw_img in enumerate(vid):\n",
    "    # Display the frame until new frame is available\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Face detection\n",
    "    refind_bboxes = tiny_face_detector(raw_img)\n",
    "\n",
    "    # Turn off the axis\n",
    "    plt.axis('off')\n",
    "    # Display the frame\n",
    "    plt.suptitle('')\n",
    "\n",
    "    # There are some detected faces!\n",
    "    for r in refind_bboxes:\n",
    "        face_img = cv2.cvtColor(raw_img[r[1]:r[3], r[0]:r[2]], cv2.COLOR_BGR2RGB)\n",
    "#         prob, label = predict_emotion_from_image(model, face_img)\n",
    "#         emo_count[label[0]] += 1\n",
    "#         # Show emotion label\n",
    "#         plt.suptitle(get_emotion(label))\n",
    "        \n",
    "        cv2.rectangle(raw_img, (r[0],r[1]), (r[2],r[3]), (0,255,0),3)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "# Final emotion\n",
    "# Turn off the axis\n",
    "plt.axis('off')\n",
    "# Display the frame\n",
    "plt.suptitle('final emotion for the video: ' + get_emotion(emo_count))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "vid.close()\n",
    "\n",
    "# return emo_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     emo_count = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
